{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('bfly': conda)"
  },
  "interpreter": {
   "hash": "ca02e749da3788ea022b22c95213f792326bd860e0c516862d3d59c40b60f658"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kmeans로 뉴스데이터 clustering 하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 형태소 분리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd     # csv 파일 편집\r\n",
    "import MeCab            # 전처리\r\n",
    "import re               # 정규 표현식 (전처리)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "전처리된 CSV 파일을 만든다\n",
    "\n",
    "이미 만들었다면 PASS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "csv_test = pd.read_csv('./SampleData/etoday.csv')\r\n",
    "cnt = 0\r\n",
    "m = MeCab.Tagger()\r\n",
    "\r\n",
    "for line in csv_test[\"Contents\"]:\r\n",
    "    remove_email = re.compile(r'\\(([^)]+\\)) | [^가-힣]+@yna\\.co\\.kr')     # 괄호 및 안의 내용 제거 | 이메일 주소 제거\r\n",
    "    remove_special_char = re.compile(r'[^가-힣^A-z^0-9^ ]') # 한글, 영어, 기본 문자를 제외한 문자들 제거\r\n",
    "    text = remove_email.sub(' ', line)\r\n",
    "    text = remove_special_char.sub(' ', text)\r\n",
    "    \r\n",
    "    tagged = m.parse(text)\r\n",
    "    s = tagged.split('\\n')\r\n",
    "\r\n",
    "    result = []\r\n",
    "    \r\n",
    "    for words in s:\r\n",
    "        # MeCab 으로 형태소 분리되면 끝이 EOS 임.\r\n",
    "        if words == 'EOS':\r\n",
    "            break\r\n",
    "        word, tag = words.split(',')[0].split('\\t')\r\n",
    "        d_tag = tag.split('+')\r\n",
    "        if (d_tag[-1] != \"\"):\r\n",
    "            tag = d_tag[0]\r\n",
    "\r\n",
    "        if tag in [\"NNB\", \"NNBC\", \"VV\", \"VA\", \"VX\", \"VCP\", \"VCN\", \"VSV\", \"MAG\", \"MAJ\", \"JKS\", \"JKC\", \"JKG\", \r\n",
    "                    \"JKO\", \"JKB\", \"JKV\", \"JKQ\", \"JC\", \"JX\", \"EP\", \"EF\", \"EC\", \"ETN\", \"ETM\", \"XPN\", \"XSN\", \r\n",
    "                    \"XSV\", \"XSA\", \"SF\", \"SE\", \"SSO\", \"SSC\", \"SC\", \"SY\", \"SH\", \"SL\", \"SN\", \"UNA\", \"NA\"]:\r\n",
    "            pass\r\n",
    "        else:\r\n",
    "            result.append(word)\r\n",
    "    sss = ' '.join(result)\r\n",
    "    # 데이터 프레임으로 만들고,\r\n",
    "    df = pd.DataFrame({\r\n",
    "        'CNo' : csv_test['CNo'][cnt],\r\n",
    "        'Subject' : csv_test['Subject'][cnt],\r\n",
    "        'Contents' : [sss]\r\n",
    "    })\r\n",
    "    # 바로 저장\r\n",
    "    df.to_csv(\"./pretreatment_data/etoday_pre.csv\", encoding='utf-8',mode = 'a', index=False, header=False)\r\n",
    "    cnt += 1\r\n",
    "    # 얼마나 했는지 확인용\r\n",
    "    if cnt % 1000 == 0:\r\n",
    "        print(cnt, \" 완료\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 단어 임베딩\n",
    "\n",
    "문서간에 비교를 위해, 단어를 임베딩하여 벡터화 시킬거임\n",
    "\n",
    "두가지 방법으로 진행해보았다.\n",
    "\n",
    "1. CountVectorizer\n",
    "2. TfidfVectorizer\n",
    "\n",
    "예상으로는 TfidfVectorizer 가 더 성능이 좋게 나올 것 같다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "et_df = pd.read_csv(\"./pretreatment_data/etoday_pre.csv\", names = [\"CNo\", \"Subject\", \"Contents\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) CountVectorizer\n",
    "\n",
    ": 단어들의 카운트(출현빈도(frequency))로 여러 문서들을 벡터화\n",
    "\n",
    "카운트 행렬, 단어 문서 행렬"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.preprocessing import normalize\r\n",
    "from sklearn.cluster import KMeans\r\n",
    "\r\n",
    "# deep copy\r\n",
    "count_df = et_df[:]\r\n",
    "\r\n",
    "n_cluster = 100\r\n",
    "\r\n",
    "vectorizer = CountVectorizer()\r\n",
    "X = vectorizer.fit_transform(count_df[\"Contents\"])\r\n",
    "\r\n",
    "X = normalize(X)\r\n",
    "\r\n",
    "kmeans = KMeans(n_clusters=n_cluster).fit(X)\r\n",
    "\r\n",
    "labels = kmeans.labels_\r\n",
    "centers = kmeans.cluster_centers_\r\n",
    "\r\n",
    "count_df[\"labels\"] = labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_df.loc[count_df['labels'] == 55, ['Subject', 'labels']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_df.loc[count_df['labels'] == 66].index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) TfidfVectorizer()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.preprocessing import normalize\r\n",
    "from sklearn.cluster import KMeans\r\n",
    "\r\n",
    "#deep copy\r\n",
    "tfidf_df = et_df[:]\r\n",
    "tfidf = TfidfVectorizer()\r\n",
    "\r\n",
    "tfidf_matrix = tfidf.fit_transform(tfidf_df['Contents'])\r\n",
    "\r\n",
    "tfidf_matrix = normalize(tfidf_matrix)\r\n",
    "\r\n",
    "kmeans_tfidf = KMeans(n_clusters=100).fit(tfidf_matrix)\r\n",
    "\r\n",
    "labels = kmeans_tfidf.labels_\r\n",
    "centers = kmeans_tfidf.cluster_centers_\r\n",
    "\r\n",
    "tfidf_df[\"labels\"] = labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tfidf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tfidf_df.loc[tfidf_df['labels'] == 55, ['Subject', 'labels']])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(kmeans_tfidf.score(tfidf_matrix))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tfidf_matrix.shape[0] -> 전체 문서 갯수\r\n",
    "\r\n",
    "im_word = []\r\n",
    "voca = sorted(tfidf.vocabulary_)\r\n",
    "for i in range(tfidf_matrix.shape[0]):\r\n",
    "    # i 번째 문서에서 가중치가 가장 높은 단어를 추출\r\n",
    "    value = tfidf_matrix[i].argmax()\r\n",
    "    im_word.append(voca[value])\r\n",
    "print(set(im_word))"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label55 = tfidf_df.loc[tfidf_df['labels'] == 55].index\r\n",
    "word55 = []\r\n",
    "voca = sorted(tfidf.vocabulary_)\r\n",
    "for i in label55:\r\n",
    "    # i 번째 문서에서 가중치가 가장 높은 단어를 추출\r\n",
    "    value = tfidf_matrix[i].argmax()\r\n",
    "    word55.append(voca[value])\r\n",
    "print(word55)\r\n",
    "print(set(word55))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set(im_word))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(type(tfidf_matrix[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sorted(tfidf.vocabulary_)[10488]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tfidf_matrix[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tfidf_matrix[1].argmax()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tfidf_matrix[0].argmin()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 적절한 k 값 찾기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"왜곡 : {}\".format(kmeans_tfidf.inertia_))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline \r\n",
    "from sklearn.cluster import KMeans\r\n",
    "\r\n",
    "# we are usingh\r\n",
    "df=et_df[:]\r\n",
    "\r\n",
    "print(df.head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "distortions = []\r\n",
    "elbow_tfidf = TfidfVectorizer()\r\n",
    "\r\n",
    "elbow_tfidf_matrix = elbow_tfidf.fit_transform(df['Contents'])\r\n",
    "\r\n",
    "elbow_tfidf_matrix = normalize(elbow_tfidf_matrix)\r\n",
    "K = range(1,10)\r\n",
    "for k in K:\r\n",
    "    kmeanModel = KMeans(n_clusters=k)\r\n",
    "    kmeanModel.fit(elbow_tfidf_matrix)\r\n",
    "    distortions.append(kmeanModel.inertia_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(16,8))\r\n",
    "plt.plot(K, distortions, 'bx-')\r\n",
    "plt.xlabel('k')\r\n",
    "plt.ylabel('Distortion')\r\n",
    "plt.title('The Elbow Method showing the optimal k')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "K = range(10,50)\r\n",
    "for k in K:\r\n",
    "    kmeanModel = KMeans(n_clusters=k)\r\n",
    "    kmeanModel.fit(elbow_tfidf_matrix)\r\n",
    "    distortions.append(kmeanModel.inertia_)\r\n",
    "plt.figure(figsize=(16,8))\r\n",
    "plt.plot(K, distortions, 'bx-')\r\n",
    "plt.xlabel('k')\r\n",
    "plt.ylabel('Distortion')\r\n",
    "plt.title('The Elbow Method showing the optimal k')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(16,8))\r\n",
    "plt.plot(K, distortions[9:50], 'bx-')\r\n",
    "plt.xlabel('k')\r\n",
    "plt.ylabel('Distortion')\r\n",
    "plt.title('The Elbow Method showing the optimal k')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "K = range(1,50)\r\n",
    "plt.figure(figsize=(16,8))\r\n",
    "plt.plot(K, distortions[:50], 'bx-')\r\n",
    "plt.xlabel('k')\r\n",
    "plt.ylabel('Distortion')\r\n",
    "plt.title('The Elbow Method showing the optimal k')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 유사도 비교\n",
    "\n",
    "바자도 KNN 이 뽑아낸 값과 비교하기 위해 사용."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "\r\n",
    "NN_df = pd.read_csv(\"./pretreatment_data/etoday_pre.csv\", names=[\"CNo\", \"Subjects\", \"Contents\"])\r\n",
    "\r\n",
    "tfidf = TfidfVectorizer()\r\n",
    "\r\n",
    "tfidf_matrix = tfidf.fit_transform(NN_df['Contents'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "source": [
    "def clac_sim(num):\r\n",
    "    # target_matrix에는 문서 하나(이투데이)의 TF-IDF 값이 저장된다.\r\n",
    "    target_matrix = tfidf.transform([NN_df._get_value(num, 'Contents')]).astype(np.float16)\r\n",
    "    # cosin_matrix에는 유사도 결과 값들이 저장된다.\r\n",
    "    cosine_matrix = cosine_similarity(target_matrix, tfidf_matrix)\r\n",
    "\r\n",
    "# 보기 쉽게 맵핑하는 부분\r\n",
    "    # news title과 id를 맵핑할 dictionary를 생성\r\n",
    "    news2id = {}\r\n",
    "    for i, c in enumerate(NN_df['CNo']):\r\n",
    "        news2id[i] = c\r\n",
    "\r\n",
    "    # id와 news title를 매핑할 dictionary를 생성\r\n",
    "    id2news = {}\r\n",
    "    for i, c in news2id.items():\r\n",
    "        id2news[c] = i\r\n",
    "# 맵핑 끝\r\n",
    "    sim_scores = [(i, c) for i, c in enumerate(cosine_matrix[0])]\r\n",
    "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True)\r\n",
    "    # sim_scores 에는 문서 번호(CNo) 와 유사도 값이 리스트 형태로 저장됨\r\n",
    "    # 5개 저장\r\n",
    "    sim_scores = [(news2id[i], round(score, 4)) for i, score in sim_scores if score >= 0.7]\r\n",
    "# \r\n",
    "    return sim_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "source": [
    "clac_sim(8080)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('E_1955225_20201027_120103.txt', 1.0),\n",
       " ('E_1953959_20201023_120202.txt', 0.844),\n",
       " ('E_1953430_20201022_120103.txt', 0.8351),\n",
       " ('E_1954665_20201026_120103.txt', 0.819),\n",
       " ('E_1951296_20201016_120102.txt', 0.8162),\n",
       " ('E_1955802_20201028_120101.txt', 0.8122),\n",
       " ('E_1949265_20201012_120102.txt', 0.8049),\n",
       " ('E_1948462_20201008_120102.txt', 0.8044),\n",
       " ('E_1947920_20201007_120103.txt', 0.7982),\n",
       " ('E_1956707_20201030_090403.txt', 0.795),\n",
       " ('E_1952934_20201021_120102.txt', 0.7932),\n",
       " ('E_1950325_20201014_120203.txt', 0.7882),\n",
       " ('E_1950655_20201015_090403.txt', 0.7848),\n",
       " ('E_1950850_20201015_120102.txt', 0.7829),\n",
       " ('E_1952466_20201020_120103.txt', 0.7818),\n",
       " ('E_1955078_20201027_090404.txt', 0.7789),\n",
       " ('E_1954060_20201023_153402.txt', 0.7755),\n",
       " ('E_1954495_20201026_090402.txt', 0.7724),\n",
       " ('E_1949786_20201013_120102.txt', 0.7692),\n",
       " ('E_1946886_20201005_120102.txt', 0.7683),\n",
       " ('E_1952785_20201021_090401.txt', 0.7676),\n",
       " ('E_1951163_20201016_090502.txt', 0.767),\n",
       " ('E_1951903_20201019_120103.txt', 0.7617),\n",
       " ('E_1947251_20201006_090403.txt', 0.7555),\n",
       " ('E_1953268_20201022_090403.txt', 0.7553),\n",
       " ('E_1952286_20201020_090403.txt', 0.7552),\n",
       " ('E_1947454_20201006_120104.txt', 0.7506),\n",
       " ('E_1956838_20201030_120103.txt', 0.7505),\n",
       " ('E_1949396_20201012_153403.txt', 0.7498),\n",
       " ('E_1956202_20201029_090402.txt', 0.7485),\n",
       " ('E_1955586_20201028_090402.txt', 0.7481),\n",
       " ('E_1956371_20201029_120102.txt', 0.7468),\n",
       " ('E_1946753_20201005_090403.txt', 0.7442),\n",
       " ('E_1947453_20201006_120103.txt', 0.7436),\n",
       " ('E_1953838_20201023_090402.txt', 0.743),\n",
       " ('E_1950170_20201014_090403.txt', 0.7402),\n",
       " ('E_1955347_20201027_153402.txt', 0.7365),\n",
       " ('E_1952583_20201020_153403.txt', 0.7352),\n",
       " ('E_1948081_20201007_153403.txt', 0.735),\n",
       " ('E_1952935_20201021_120102.txt', 0.7337),\n",
       " ('E_1953431_20201022_120103.txt', 0.7314),\n",
       " ('E_1953960_20201023_120203.txt', 0.7275),\n",
       " ('E_1951297_20201016_120102.txt', 0.7243),\n",
       " ('E_1952053_20201019_153402.txt', 0.7212),\n",
       " ('E_1951722_20201019_090403.txt', 0.7202),\n",
       " ('E_1949637_20201013_090402.txt', 0.7134),\n",
       " ('E_1950450_20201014_153404.txt', 0.7112),\n",
       " ('E_1951904_20201019_120104.txt', 0.7107),\n",
       " ('E_1951723_20201019_090404.txt', 0.7091),\n",
       " ('E_1953560_20201022_153403.txt', 0.7079),\n",
       " ('E_1954666_20201026_120104.txt', 0.7077),\n",
       " ('E_1952786_20201021_090402.txt', 0.7071),\n",
       " ('E_1949923_20201013_153402.txt', 0.7069),\n",
       " ('E_1951388_20201016_153402.txt', 0.706),\n",
       " ('E_1953027_20201021_153403.txt', 0.7048),\n",
       " ('E_1946887_20201005_120103.txt', 0.7037),\n",
       " ('E_1956496_20201029_153403.txt', 0.7014),\n",
       " ('E_1946754_20201005_090404.txt', 0.7009),\n",
       " ('E_1956839_20201030_120104.txt', 0.7002)]"
      ]
     },
     "metadata": {},
     "execution_count": 206
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 비지도 KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.neighbors import NearestNeighbors\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# 사용 데이터 불러오기\r\n",
    "NN_df = pd.read_csv(\"./pretreatment_data/etoday_pre.csv\", names=[\"CNo\", \"Subjects\", \"Contents\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.preprocessing import normalize\r\n",
    "# Tfidf로 데이터 임베딩\r\n",
    "nn_tf = TfidfVectorizer()\r\n",
    "\r\n",
    "nn_tf_matrix = nn_tf.fit_transform(NN_df['Contents'])\r\n",
    "\r\n",
    "nn_tf_matrix = normalize(nn_tf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "source": [
    "# 이웃 검색을 위한 비지도 학습기\r\n",
    "# n_neighbors = 이웃수\r\n",
    "# radius = 반경\r\n",
    "\r\n",
    "neigh = NearestNeighbors(n_neighbors=10, radius=0.78)\r\n",
    "\r\n",
    "neigh.fit(nn_tf_matrix)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=10, radius=0.78)"
      ]
     },
     "metadata": {},
     "execution_count": 215
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "print(nn_tf_matrix[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 26938)\t0.020609648026551138\n",
      "  (0, 36657)\t0.02726429180559092\n",
      "  (0, 3886)\t0.02101533056723216\n",
      "  (0, 31657)\t0.04506928394524605\n",
      "  (0, 38381)\t0.04142633763737896\n",
      "  (0, 4188)\t0.045243715926478974\n",
      "  (0, 24538)\t0.03276154750122785\n",
      "  (0, 21667)\t0.03441002516659542\n",
      "  (0, 18085)\t0.04046811741980474\n",
      "  (0, 4561)\t0.04198007674542899\n",
      "  (0, 32186)\t0.02324455705596447\n",
      "  (0, 21364)\t0.021921897685280217\n",
      "  (0, 32586)\t0.024008100922409896\n",
      "  (0, 35867)\t0.03175303349724676\n",
      "  (0, 37918)\t0.03318222480407057\n",
      "  (0, 4309)\t0.028151965184440512\n",
      "  (0, 26070)\t0.026789263857342076\n",
      "  (0, 35604)\t0.02999658075961418\n",
      "  (0, 9132)\t0.04198007674542899\n",
      "  (0, 22510)\t0.03895615809418049\n",
      "  (0, 7249)\t0.017973194703385258\n",
      "  (0, 14015)\t0.01937517804311522\n",
      "  (0, 6610)\t0.015201894346838855\n",
      "  (0, 3168)\t0.013627695038187289\n",
      "  (0, 14195)\t0.028871811228738848\n",
      "  :\t:\n",
      "  (0, 6969)\t0.03752696678735667\n",
      "  (0, 9913)\t0.02798641888188761\n",
      "  (0, 14014)\t0.029068307406953903\n",
      "  (0, 15354)\t0.04506928394524605\n",
      "  (0, 19227)\t0.03329287437671011\n",
      "  (0, 36396)\t0.05294891911252672\n",
      "  (0, 32053)\t0.03583057939576037\n",
      "  (0, 22840)\t0.18968610166959626\n",
      "  (0, 2983)\t0.060339847196550875\n",
      "  (0, 22223)\t0.11085950569291124\n",
      "  (0, 36275)\t0.049884997017781735\n",
      "  (0, 34363)\t0.03237206115386534\n",
      "  (0, 28994)\t0.1146915633026533\n",
      "  (0, 34584)\t0.03682517111193765\n",
      "  (0, 21588)\t0.046242050709914655\n",
      "  (0, 9586)\t0.046242050709914655\n",
      "  (0, 10301)\t0.04198007674542899\n",
      "  (0, 23271)\t0.04259910440204756\n",
      "  (0, 30216)\t0.018221442055689093\n",
      "  (0, 24195)\t0.04330090007746658\n",
      "  (0, 1821)\t0.03728247337534834\n",
      "  (0, 1545)\t0.021113570073721635\n",
      "  (0, 16058)\t0.05755250226243034\n",
      "  (0, 4178)\t0.15244338675324676\n",
      "  (0, 16919)\t0.6052206136012211\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "# 점의 지정된 반경 내에서 이웃을 찾습니다.\r\n",
    "nbrs = neigh.radius_neighbors(nn_tf_matrix[8080], sort_results = True)\r\n",
    "print(len(nbrs[0][0]))\r\n",
    "\r\n",
    "me = nbrs[1][0][0]\r\n",
    "\r\n",
    "for n, i in enumerate(nbrs[1][0]):\r\n",
    "    cosine_matrix = cosine_similarity(nn_tf_matrix[me], nn_tf_matrix[i])\r\n",
    "    print('거리 : {: .4f} / 제목 : {} '.format(nbrs[0][0][n], NN_df[\"Subjects\"][i]))\r\n",
    "    print(\"유사도 : \", round(cosine_matrix[0][0], 4))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "61\n",
      "거리 :  0.0000 / 제목 : [시황_정오] 코스피 2344.84p, 상승세 (▲0.93p, +0.04%) 반전 \n",
      "유사도 :  1.0\n",
      "거리 :  0.5586 / 제목 : [시황_정오] 코스피 2365.84p, 상승세 (▲10.79p, +0.46%) 지속 \n",
      "유사도 :  0.844\n",
      "거리 :  0.5743 / 제목 : [시황_정오] 코스피 2353.07p, 하락세 (▼17.79p, -0.75%) 지속 \n",
      "유사도 :  0.8351\n",
      "거리 :  0.6016 / 제목 : [시황_정오] 코스피 2356.9p, 하락세 (▼3.91p, -0.17%) 반전 \n",
      "유사도 :  0.819\n",
      "거리 :  0.6063 / 제목 : [시황_정오] 코스피 2351.94p, 하락세 (▼9.27p, -0.39%) 반전 \n",
      "유사도 :  0.8162\n",
      "거리 :  0.6129 / 제목 : [시황_정오] 코스피 2328.77p, 하락세 (▼2.07p, -0.09%) 지속 \n",
      "유사도 :  0.8122\n",
      "거리 :  0.6247 / 제목 : [시황_정오] 코스피 2404.53p, 상승세 (▲12.57p, +0.53%) 지속 \n",
      "유사도 :  0.8049\n",
      "거리 :  0.6254 / 제목 : [시황_정오] 코스피 2394.66p, 상승세 (▲7.72p, +0.32%) 지속 \n",
      "유사도 :  0.8044\n",
      "거리 :  0.6353 / 제목 : [시황_정오] 코스피 2371.21p, 상승세 (▲5.31p, +0.22%) 반전 \n",
      "유사도 :  0.7982\n",
      "거리 :  0.6404 / 제목 : [시황_개장] 코스피 2312.86p, 외국인 순매도에 하락세 (▼13.81p, -0.59%) \n",
      "유사도 :  0.795\n",
      "거리 :  0.6432 / 제목 : [시황_정오] 코스피 2364.04p, 상승세 (▲5.63p, +0.24%) 지속 \n",
      "유사도 :  0.7932\n",
      "거리 :  0.6509 / 제목 : [시황_정오] 코스피 2384.78p, 하락세 (▼18.37p, -0.76%) 지속 \n",
      "유사도 :  0.7882\n",
      "거리 :  0.6560 / 제목 : [시황_개장] 코스피 2375.29p, 개인 순매도에 하락세 (▼5.19p, -0.22%) \n",
      "유사도 :  0.7848\n",
      "거리 :  0.6589 / 제목 : [시황_정오] 코스피 2361.21p, 하락세 (▼19.27p, -0.81%) 지속 \n",
      "유사도 :  0.7829\n",
      "거리 :  0.6607 / 제목 : [시황_정오] 코스피 2328.12p, 하락세 (▼18.62p, -0.79%) 지속 \n",
      "유사도 :  0.7818\n",
      "거리 :  0.6649 / 제목 : [시황_개장] 코스피 2322.68p, 기관 순매도에 하락세 (▼21.23p, -0.91%) \n",
      "유사도 :  0.7789\n",
      "거리 :  0.6700 / 제목 : [시황_장마감] 코스피 2360.81p, 상승(▲5.76p, +0.24%)마감. 기관 +2246억, 외국인 +381억, 개인 -2705억 \n",
      "유사도 :  0.7755\n",
      "거리 :  0.6746 / 제목 : [시황_개장] 코스피 2370.12p, 외국인 순매수에 상승세 (▲9.31p, +0.39%) \n",
      "유사도 :  0.7724\n",
      "거리 :  0.6794 / 제목 : [시황_정오] 코스피 2391.21p, 하락세 (▼12.52p, -0.52%) 반전 \n",
      "유사도 :  0.7692\n",
      "거리 :  0.6808 / 제목 : [시황_정오] 코스피 2357.5p, 상승세 (▲29.61p, +1.27%) 지속 \n",
      "유사도 :  0.7683\n",
      "거리 :  0.6818 / 제목 : [시황_개장] 코스피 2365.89p, 개인 순매수에 상승세 (▲7.48p, +0.32%) \n",
      "유사도 :  0.7676\n",
      "거리 :  0.6827 / 제목 : [시황_개장] 코스피 2361.37p, 개인 순매수에 상승세 (▲0.16p, +0.01%) \n",
      "유사도 :  0.767\n",
      "거리 :  0.6904 / 제목 : [시황_정오] 코스피 2359.78p, 상승세 (▲18.25p, +0.78%) 지속 \n",
      "유사도 :  0.7617\n",
      "거리 :  0.6992 / 제목 : [시황_개장] 코스피 2369.37p, 개인 순매수에 상승세 (▲11.37p, +0.48%) \n",
      "유사도 :  0.7555\n",
      "거리 :  0.6996 / 제목 : [시황_개장] 코스피 2352.3p, 외국인 순매도에 하락세 (▼18.56p, -0.78%) \n",
      "유사도 :  0.7553\n",
      "거리 :  0.6997 / 제목 : [시황_개장] 코스피 2337.36p, 개인 순매도에 하락세 (▼9.38p, -0.40%) \n",
      "유사도 :  0.7552\n",
      "거리 :  0.7063 / 제목 : [시황_정오] 코스닥 864.81p, 상승세 (▲6.42p, +0.75%) 지속 \n",
      "유사도 :  0.7506\n",
      "거리 :  0.7064 / 제목 : [시황_정오] 코스피 2301.4p, 하락세 (▼25.27p, -1.09%) 지속 \n",
      "유사도 :  0.7505\n",
      "거리 :  0.7075 / 제목 : [시황_장마감] 코스피 2403.73p, 상승(▲11.77p, +0.49%)마감. 외국인 +1379억, 기관 +840억, 개인 -2342억 \n",
      "유사도 :  0.7498\n",
      "거리 :  0.7092 / 제목 : [시황_개장] 코스피 2313.88p, 개인 순매도에 하락세 (▼31.38p, -1.34%) \n",
      "유사도 :  0.7485\n",
      "거리 :  0.7098 / 제목 : [시황_개장] 코스피 2323.02p, 기관 순매도에 하락세 (▼7.82p, -0.34%) \n",
      "유사도 :  0.7481\n",
      "거리 :  0.7117 / 제목 : [시황_정오] 코스피 2305.79p, 하락세 (▼39.47p, -1.68%) 지속 \n",
      "유사도 :  0.7468\n",
      "거리 :  0.7153 / 제목 : [시황_개장] 코스피 2327.9p, 기관 순매수에 상승세 (▲0.01p, +0.00%) \n",
      "유사도 :  0.7442\n",
      "거리 :  0.7161 / 제목 : [시황_정오] 코스피 2370.14p, 상승세 (▲12.14p, +0.51%) 지속 \n",
      "유사도 :  0.7436\n",
      "거리 :  0.7169 / 제목 : [시황_개장] 코스피 2364.17p, 개인 순매수에 상승세 (▲9.12p, +0.39%) \n",
      "유사도 :  0.743\n",
      "거리 :  0.7208 / 제목 : [시황_개장] 코스피 2403.06p, 기관 순매도에 하락세 (▼0.09p, -0.00%) \n",
      "유사도 :  0.7402\n",
      "거리 :  0.7260 / 제목 : [시황_장마감] 코스피 2330.84p, 하락(▼13.07p, -0.56%)마감. 개인 +1061억, 외국인 +1044억, 기관 -2486억 \n",
      "유사도 :  0.7365\n",
      "거리 :  0.7278 / 제목 : [시황_장마감] 코스피 2358.41p, 상승(▲11.67p, +0.50%)마감. 기관 +2593억, 외국인 +414억, 개인 -2884억 \n",
      "유사도 :  0.7352\n",
      "거리 :  0.7280 / 제목 : [시황_장마감] 코스피 2386.94p, 상승(▲21.04p, +0.89%)마감. 기관 +4292억, 외국인 -939억, 개인 -3465억 \n",
      "유사도 :  0.735\n",
      "거리 :  0.7299 / 제목 : [시황_정오] 코스닥 825.56p, 상승세 (▲0.91p, +0.11%) 지속 \n",
      "유사도 :  0.7337\n",
      "거리 :  0.7330 / 제목 : [시황_정오] 코스닥 820.35p, 하락세 (▼10.32p, -1.24%) 지속 \n",
      "유사도 :  0.7314\n",
      "거리 :  0.7382 / 제목 : [시황_정오] 코스닥 813.25p, 상승세 (▲0.55p, +0.07%) 지속 \n",
      "유사도 :  0.7275\n",
      "거리 :  0.7425 / 제목 : [시황_정오] 코스닥 835.92p, 하락세 (▼8.52p, -1.01%) 반전 \n",
      "유사도 :  0.7243\n",
      "거리 :  0.7468 / 제목 : [시황_장마감] 코스피 2346.74p, 상승(▲5.21p, +0.22%)마감. 기관 +2965억, 외국인 +435억, 개인 -3511억 \n",
      "유사도 :  0.7212\n",
      "거리 :  0.7481 / 제목 : [시황_개장] 코스피 2355.87p, 기관 순매수에 상승세 (▲14.34p, +0.61%) \n",
      "유사도 :  0.7202\n",
      "거리 :  0.7571 / 제목 : [시황_개장] 코스피 2418.89p, 외국인 순매수에 상승세 (▲15.16p, +0.63%) \n",
      "유사도 :  0.7134\n",
      "거리 :  0.7600 / 제목 : [시황_장마감] 코스피 2380.48p, 하락(▼22.67p, -0.94%)마감. 개인 +4144억, 외국인 -374억, 기관 -3758억 \n",
      "유사도 :  0.7112\n",
      "거리 :  0.7606 / 제목 : [시황_정오] 코스닥 834.61p, 상승세 (▲0.77p, +0.09%) 지속 \n",
      "유사도 :  0.7107\n",
      "거리 :  0.7628 / 제목 : [시황_개장] 코스닥 837.1p, 외국인 순매수에 상승세 (▲3.26p, +0.39%) \n",
      "유사도 :  0.7091\n",
      "거리 :  0.7643 / 제목 : [시황_장마감] 코스피 2355.05p, 하락(▼15.81p, -0.67%)마감. 개인 +670억, 기관 +81억, 외국인 -734억 \n",
      "유사도 :  0.7079\n",
      "거리 :  0.7646 / 제목 : [시황_정오] 코스닥 789.34p, 하락세 (▼18.64p, -2.31%) 지속 \n",
      "유사도 :  0.7077\n",
      "거리 :  0.7654 / 제목 : [시황_개장] 코스닥 827.9p, 개인 순매수에 상승세 (▲3.25p, +0.39%) \n",
      "유사도 :  0.7071\n",
      "거리 :  0.7657 / 제목 : [시황_장마감] 코스피 2403.15p, 하락(▼0.58p, -0.02%)마감. 외국인 +2089억, 개인 -333억, 기관 -1718억 \n",
      "유사도 :  0.7069\n",
      "거리 :  0.7668 / 제목 : [시황_장마감] 코스피 2341.53p, 하락(▼19.68p, -0.83%)마감. 개인 +4905억, 외국인 -1731억, 기관 -2024억 \n",
      "유사도 :  0.706\n",
      "거리 :  0.7684 / 제목 : [시황_장마감] 코스피 2370.86p, 상승(▲12.45p, +0.53%)마감. 기관 +1146억, 외국인 +64억, 개인 -1314억 \n",
      "유사도 :  0.7048\n",
      "거리 :  0.7698 / 제목 : [시황_정오] 코스닥 855.38p, 상승세 (▲7.23p, +0.85%) 반전 \n",
      "유사도 :  0.7037\n",
      "거리 :  0.7727 / 제목 : [시황_장마감] 코스피 2326.67p, 하락(▼18.59p, -0.79%)마감. 개인 +9804억, 기관 -4763억, 외국인 -5357억 \n",
      "유사도 :  0.7014\n",
      "거리 :  0.7734 / 제목 : [시황_개장] 코스닥 847.96p, 외국인 순매도에 하락세 (▼0.19p, -0.02%) \n",
      "유사도 :  0.7009\n",
      "거리 :  0.7744 / 제목 : [시황_정오] 코스닥 806.19p, 하락세 (▼7.74p, -0.95%) 지속 \n",
      "유사도 :  0.7002\n",
      "거리 :  0.7766 / 제목 : [시황_정오] 코스닥 866.84p, 상승세 (▲4.26p, +0.49%) 반전 \n",
      "유사도 :  0.6985\n",
      "거리 :  0.7784 / 제목 : [시황_정오] 코스닥 844.16p, 하락세 (▼17.32p, -2.01%) 지속 \n",
      "유사도 :  0.6971\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "nbrs[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.44764929, 0.73577991])"
      ]
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "source": [
    "# kneighbors : 점의 k-이웃을 찾는다.\r\n",
    "# \r\n",
    "nei_list = neigh.kneighbors(nn_tf_matrix[8080])\r\n",
    "print(nei_list[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[8080 6933 6442 7573 4519 8609 2688 1986 1510 9407]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "# me 에 기준점을 저장\r\n",
    "me = nei_list[1][0][0]\r\n",
    "\r\n",
    "for n, i in enumerate(nei_list[1][0]):\r\n",
    "    # 기준점과 이웃되는 점의 코사인 유사도를 계산\r\n",
    "    cosine_matrix = cosine_similarity(nn_tf_matrix[me], nn_tf_matrix[i])\r\n",
    "    print('거리 : {: .4f} / 제목 : {} '.format(nei_list[0][0][n], NN_df[\"Subjects\"][i]))\r\n",
    "    print(\"코사인 유사도 : \", round(cosine_matrix[0][0], 4))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "거리 :  0.0000 / 제목 : [시황_정오] 코스피 2344.84p, 상승세 (▲0.93p, +0.04%) 반전 \n",
      "코사인 유사도 :  1.0\n",
      "거리 :  0.5586 / 제목 : [시황_정오] 코스피 2365.84p, 상승세 (▲10.79p, +0.46%) 지속 \n",
      "코사인 유사도 :  0.844\n",
      "거리 :  0.5743 / 제목 : [시황_정오] 코스피 2353.07p, 하락세 (▼17.79p, -0.75%) 지속 \n",
      "코사인 유사도 :  0.8351\n",
      "거리 :  0.6016 / 제목 : [시황_정오] 코스피 2356.9p, 하락세 (▼3.91p, -0.17%) 반전 \n",
      "코사인 유사도 :  0.819\n",
      "거리 :  0.6063 / 제목 : [시황_정오] 코스피 2351.94p, 하락세 (▼9.27p, -0.39%) 반전 \n",
      "코사인 유사도 :  0.8162\n",
      "거리 :  0.6129 / 제목 : [시황_정오] 코스피 2328.77p, 하락세 (▼2.07p, -0.09%) 지속 \n",
      "코사인 유사도 :  0.8122\n",
      "거리 :  0.6247 / 제목 : [시황_정오] 코스피 2404.53p, 상승세 (▲12.57p, +0.53%) 지속 \n",
      "코사인 유사도 :  0.8049\n",
      "거리 :  0.6254 / 제목 : [시황_정오] 코스피 2394.66p, 상승세 (▲7.72p, +0.32%) 지속 \n",
      "코사인 유사도 :  0.8044\n",
      "거리 :  0.6353 / 제목 : [시황_정오] 코스피 2371.21p, 상승세 (▲5.31p, +0.22%) 반전 \n",
      "코사인 유사도 :  0.7982\n",
      "거리 :  0.6404 / 제목 : [시황_개장] 코스피 2312.86p, 외국인 순매도에 하락세 (▼13.81p, -0.59%) \n",
      "코사인 유사도 :  0.795\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "nei_list[1][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([   0, 5928, 4836, 1724, 3577], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "nei_list[0][0][2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0220781561234535"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "시각화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "w_list = []\r\n",
    "w_list.append([])\r\n",
    "for i in  nn_tf.vocabulary_.keys():\r\n",
    "    w_list.append(nn_tf.vocabulary_[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "from sklearn.decomposition import PCA\r\n",
    "\r\n",
    "pca = PCA(n_components=3)\r\n",
    "pca_result = pca.fit_transform(w_list)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-10782be2e9f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpca_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bfly\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mC\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[1;34m'np.ascontiguousarray'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \"\"\"\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bfly\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         X = self._validate_data(X, dtype=[np.float64, np.float32],\n\u001b[1;32m--> 405\u001b[1;33m                                 ensure_2d=True, copy=self.copy)\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[1;31m# Handle n_components==None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bfly\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bfly\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bfly\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\.conda\\envs\\bfly\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\r\n",
    "from sklearn.manifold import TSNE\r\n",
    "\r\n",
    "n_sne = 9000\r\n",
    "\r\n",
    "time_start = time.time()\r\n",
    "tsen = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\r\n",
    "tsen_results = tsen.fit_transform(et_df.loc[9000, tfidf.vocabulary_.keys()].values)\r\n",
    "\r\n",
    "print('t-SNE done! Time elapsed: {} second'.format(time.time() - time_start))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}