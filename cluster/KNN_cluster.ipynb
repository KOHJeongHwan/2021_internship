{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('cluster': conda)"
  },
  "interpreter": {
   "hash": "927c2d5ba6911836a076d632dd2c11484a548eae505d21962180c1153bef520a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 코사인 유사도\r\n",
    "\r\n",
    "코사인 유사도를 사용하는 이유 : 비지도 KNN 으로 그룹화된 뉴스들을 수치화하여 비교하기 위해\r\n",
    "\r\n",
    "Vectorizing 은 Tfidf 를 사용함."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "\r\n",
    "# etoday_pre.csv 는 이미 전처리된 (형태소 분리된) 파일\r\n",
    "NN_df = pd.read_csv(\"./etoday_pre.csv\", names=[\"CNo\", \"Subjects\", \"Contents\"])\r\n",
    "\r\n",
    "tfidf = TfidfVectorizer()\r\n",
    "# 전체 문서를 벡터화 시킴 = tfidf_matrix\r\n",
    "tfidf_matrix = tfidf.fit_transform(NN_df['Contents'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clac_sim(num):\r\n",
    "    # target_matrix에는 문서 하나(이투데이)의 TF-IDF 값이 저장된다.\r\n",
    "    target_matrix = tfidf.transform([NN_df._get_value(num, 'Contents')]).astype(np.float16)\r\n",
    "    # cosin_matrix에는 전체 문서와 비교할 문서의 유사도값이 저장됨.\r\n",
    "    cosine_matrix = cosine_similarity(target_matrix, tfidf_matrix)\r\n",
    "\r\n",
    "# 보기 쉽게 맵핑하는 부분\r\n",
    "    # news title과 id를 맵핑할 dictionary를 생성\r\n",
    "    news2id = {}\r\n",
    "    for i, c in enumerate(NN_df['CNo']):\r\n",
    "        news2id[i] = c\r\n",
    "\r\n",
    "    # id와 news title를 매핑할 dictionary를 생성\r\n",
    "    id2news = {}\r\n",
    "    for i, c in news2id.items():\r\n",
    "        id2news[c] = i\r\n",
    "# 맵핑 끝\r\n",
    "    sim_scores = [(i, c) for i, c in enumerate(cosine_matrix[0])]\r\n",
    "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True)\r\n",
    "    # sim_scores 에는 문서 번호(CNo) 와 유사도 값이 리스트 형태로 저장됨\r\n",
    "    \r\n",
    "    # sim_scores = [(news2id[i], round(score, 4)) for i, score in sim_scores if score >= 0.7 and score < 0.8]       # 70% 이상 80% 미만인 뉴스들을 출력\r\n",
    "    sim_scores = [(news2id[i], round(score, 4)) for i, score in sim_scores]                                         # 제한 없이 출력\r\n",
    "\r\n",
    "    return sim_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 확인.\r\n",
    "clac_sim(60)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 비지도 KNN\r\n",
    "\r\n",
    "NearestNeighbors(n_neighbors=이웃갯수, radius=반경)\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import NearestNeighbors\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# 사용 데이터 불러오기\r\n",
    "NN_df = pd.read_csv(\"./etoday_pre.csv\", names=[\"CNo\", \"Subjects\", \"Contents\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.preprocessing import normalize\r\n",
    "# Tfidf로 토큰화\r\n",
    "nn_tf = TfidfVectorizer()\r\n",
    "\r\n",
    "nn_tf_matrix = nn_tf.fit_transform(NN_df['Contents'])\r\n",
    "# 정규화\r\n",
    "nn_tf_matrix = normalize(nn_tf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 이웃 검색을 위한 비지도 학습기 객체 형성\r\n",
    "# n_neighbors = 이웃수\r\n",
    "# radius = 반경\r\n",
    "\r\n",
    "neigh = NearestNeighbors(n_neighbors=10, radius=1.0)\r\n",
    "\r\n",
    "neigh.fit(nn_tf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "cluster를 위한 새로운 label 'cluster' 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NN_df[\"cluster\"] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 확인.\r\n",
    "NN_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) 근접 이웃 개수 이용\r\n",
    "\r\n",
    "kneighbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# kneighbors : 점의 k-이웃을 찾는다.\r\n",
    "# \r\n",
    "nei_list = neigh.kneighbors(nn_tf_matrix[1004])\r\n",
    "print(nei_list[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "# me 에 기준점(클러스터 중심)을 저장\r\n",
    "me = nei_list[1][0][0]\r\n",
    "\r\n",
    "for n, i in enumerate(nei_list[1][0]):\r\n",
    "    # 기준점과 이웃되는 점의 코사인 유사도를 계산\r\n",
    "    cosine_matrix = cosine_similarity(nn_tf_matrix[me], nn_tf_matrix[i])\r\n",
    "    print('거리 : {: .4f} / 제목 : {} '.format(nei_list[0][0][n], NN_df[\"Subjects\"][i]))\r\n",
    "    print(\"코사인 유사도 : \", round(cosine_matrix[0][0], 4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) 거리(반경) 기반 cluster\r\n",
    "\r\n",
    "radius_neigbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "test 용 파일. 사용 함수는 아래에 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test 파일!!!\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "# 점의 지정된 반경 내에서 이웃을 찾습니다.\r\n",
    "nbrs = neigh.radius_neighbors(nn_tf_matrix[100], sort_results = True)     # nn_tf_matrix[index] : index 값으로 중심이 되는 뉴스 선택\r\n",
    "# print(nbrs)           # 확인용  # nbr[0][0]에 거리, nbr[1][0]에 index가 들어간다.\r\n",
    "me = nbrs[1][0][0]      # 중심 뉴스의 인덱스 = me\r\n",
    "\r\n",
    "for n, i in enumerate(nbrs[1][0]):\r\n",
    "    cosine_matrix = cosine_similarity(nn_tf_matrix[me], nn_tf_matrix[i])\r\n",
    "    if (n == 0 or (cosine_matrix[0][0] >=0.6 and cosine_matrix[0][0]<0.8)):\r\n",
    "        print('거리 : {: .4f} / 제목 : {} '.format(nbrs[0][0][n], NN_df[\"Subjects\"][i]))\r\n",
    "        print(\"index : \", i)\r\n",
    "        print(\"유사도 : \", round(cosine_matrix[0][0], 4))\r\n",
    "        print(\"-\"*50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 함수화\r\n",
    "\r\n",
    "거리 내에 있는 모든 원소를 비교하고 싶기 때문에, 2) 거리 기반 cluster 로 함수화 진행"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 위의 코드를 함수화 시키자\r\n",
    "# index : 뉴스의 index 번호, 중심이 될 뉴스의 index를 넣어라.\r\n",
    "# tfidf_matrix : 우리가 계산한 tfidf_matrix\r\n",
    "\r\n",
    "def clustering(index, tfidf_matrix):\r\n",
    "    nbrs = neigh.radius_neighbors(tfidf_matrix[index], sort_results = True)\r\n",
    "\r\n",
    "    me = nbrs[1][0][0]\r\n",
    "    clu_list = []\r\n",
    "\r\n",
    "    for n, i in enumerate(nbrs[1][0]):\r\n",
    "        cosine_matrix = cosine_similarity(tfidf_matrix[me], tfidf_matrix[i])\r\n",
    "        if (n == 0 or (cosine_matrix[0][0] >=0.6 and cosine_matrix[0][0]<0.8)):\r\n",
    "            clu_list.append(i)\r\n",
    "    return clu_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 동작부분\r\n",
    "clustering 동작 부분."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 함수를 사용해 clustering 시작\r\n",
    "cnt = 0     # 묶인 cluster 에 이름대신 숫자를 붙인다.\r\n",
    "for line in range(len(NN_df)):\r\n",
    "    # 해당 line이 클러스터가 형성되지 않았을 경우(클러스터 넘버가 0일 경우)에 함수 동작\r\n",
    "    if NN_df[\"cluster\"][line] == 0:\r\n",
    "        cnt += 1\r\n",
    "        clu_list = clustering(line, nn_tf_matrix)\r\n",
    "    for i in clu_list:\r\n",
    "        NN_df[\"cluster\"][i] = cnt   # 클러스터 숫자를 붙여준다.\r\n",
    "    print(NN_df[\"cluster\"][i])"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "결과 확인 작업 및 저장"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 출력 왼쪽이 클러스터 이름, 오른쪽이 클러스터에 소속된 뉴스 수\r\n",
    "NN_df[\"cluster\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "for i in range(len(NN_df)):\r\n",
    "    if NN_df[\"cluster\"][i] == 5337:     # 5337 부분에 클러스터 이름을 넣으면, 해당 클러스터의 속한 뉴스 index 출력\r\n",
    "        print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 확인용\r\n",
    "NN_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 저장용\r\n",
    "NN_df.to_csv(\"./clustering_etoday.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 시각화\r\n",
    "\r\n",
    "큰 의미는 없었다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 시각화, 의미 없다.\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "plt.figure(figsize=(48,5))\r\n",
    "\r\n",
    "plt.title('Cluster' , fontsize=20)\r\n",
    "\r\n",
    "plt.ylabel('member count' , fontsize=15)\r\n",
    "plt.xlabel('cluster' , fontsize=15)\r\n",
    "\r\n",
    "NN_df['cluster'].value_counts().value_counts().plot.bar()\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 워드 랭크로 주요단어 추출해보기\r\n",
    "\r\n",
    "생각보다 비슷한 뉴스가 없었나 보다.\r\n",
    "\r\n",
    "각각 클러스터된 문장들이 어떤 주제로 엮었는지 알아보자.\r\n",
    "\r\n",
    "그러기 위해서는 각 문장의 핵심 단어, 키워드를 추출할 필요가 있어 보인다!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### test 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "from textrank import KeywordSummarizer\r\n",
    "\r\n",
    "# \r\n",
    "def go(sent):\r\n",
    "    return sent\r\n",
    "\r\n",
    "clu_df = pd.read_csv(\"./clustering_etoday.csv\")\r\n",
    "\r\n",
    "\r\n",
    "keyword_extractor = KeywordSummarizer(\r\n",
    "    tokenize = go,\r\n",
    "    window = 2,\r\n",
    "    verbose = True\r\n",
    ")\r\n",
    "# keyword_extractor = summarize(\r\n",
    "#     tokenize = None,\r\n",
    "#     min_sim = 0.5,\r\n",
    "#     verbose = True\r\n",
    "# )\r\n",
    "\r\n",
    "# keyword_extractor = summarizer.KeywordSummarizer(tokenize=go, window = 2)\r\n",
    "keywords  = keyword_extractor.summarize(clu_df[\"Contents\"][0], topk=10)\r\n",
    "print(keywords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank 테스트 코드 작성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "from textrank import KeywordSummarizer          # 구현된 textrank\r\n",
    "from konlpy.tag import Komoran                  # Komoran를 사용하기 위한 import -> 이미 전처리가 되어있다면 사용하지 않아도 된다.\r\n",
    "\r\n",
    "clu_df = pd.read_csv(\"./clustering_etoday.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "komoran = Komoran()             # 코모란 객체 선언\r\n",
    "def komoran_tokenize(sent):     # 코모란 사용 함수\r\n",
    "    words = komoran.pos(sent, join=True)\r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# text_rank 테스트 코드\r\n",
    "# df 전체가 돌아간다. \r\n",
    "# 마지막에 CSV 파일이 저장된다.\r\n",
    "\r\n",
    "import re\r\n",
    "\r\n",
    "clu_df[\"textrank_output\"] = \"\"\r\n",
    "\r\n",
    "keyword_extractor = KeywordSummarizer(\r\n",
    "    tokenize= komoran_tokenize,         # 위에서 만든 코모란 함수를 넣어준다.\r\n",
    "    window= -1,\r\n",
    "    verbose =False\r\n",
    ")\r\n",
    "\r\n",
    "# 전처리\r\n",
    "for idx in range(len(clu_df)):      # 데이터의 수(index)만큼 돈다\r\n",
    "    word = clu_df['Contents'][idx].replace('\\n', '').replace('  ', '')    \r\n",
    "    result = re.sub('[(-=.\\'·ㆍ>▷▶◆…‘’“”\\\"#/?:$})]', '', word)          # 특수문자 제거\r\n",
    "    clu_df['textrank_output'][idx] = [str(result)]\r\n",
    "    print(0,idx)\r\n",
    "\r\n",
    "for idx in range(len(clu_df)):   \r\n",
    "    outputs = []\r\n",
    "    try:\r\n",
    "        sents = clu_df['textrank_output'][idx]\r\n",
    "        keywords = keyword_extractor.summarize(sents, topk=10)\r\n",
    "        for word, rank in keywords:\r\n",
    "            # outputs.append('{} ({:.3})'.format(word, rank))       # 결과 + 점수\r\n",
    "            word = word.split(\"/\")[0]\r\n",
    "            outputs.append('{}'.format(word))                       # 결과만\r\n",
    "        print(idx)\r\n",
    "        outputs = \" \".join(outputs)\r\n",
    "        clu_df['textrank_output'][idx] = outputs\r\n",
    "        \r\n",
    "    except:\r\n",
    "        continue\r\n",
    "    \r\n",
    "clu_df.to_csv('TextRank_Data.csv', encoding='utf-8-sig', mode = 'a')    # 저장"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 결과 및 점수가 저장됨.\r\n",
    "keywords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text_df = pd.read_csv(\"./TextRank_Data.csv\")\r\n",
    "text_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# clustering 결과로 CNo 랑 Subject 출력하기.\r\n",
    "for line in range(len(clu_df)):\r\n",
    "    if clu_df[\"cluster\"][line] == 4636:\r\n",
    "        print(clu_df[\"CNo\"][line], clu_df[\"Subjects\"][line])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "key_W = []\r\n",
    "for line in range(len(clu_df)):\r\n",
    "    if text_df[\"cluster\"][line] == 4636:\r\n",
    "        print(text_df[\"CNo\"][line], text_df[\"textrank_output\"][line])\r\n",
    "        test =  text_df[\"textrank_output\"][line].split()\r\n",
    "        key_W += test\r\n",
    "print(key_W)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count={}\r\n",
    "for i in key_W:\r\n",
    "    try: \r\n",
    "        count[i] += 1\r\n",
    "    except: \r\n",
    "        count[i]=1\r\n",
    "pgm_lang_val_reverse = sorted(count.items(), reverse=True, key=lambda item: item[1])\r\n",
    "\r\n",
    "print(pgm_lang_val_reverse)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank 함수화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "from textrank import KeywordSummarizer\r\n",
    "from konlpy.tag import Komoran\r\n",
    "\r\n",
    "clu_df = pd.read_csv(\"./clustering_etoday.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "komoran = Komoran()\r\n",
    "def komoran_tokenize(sent):\r\n",
    "    words = komoran.pos(sent, join=True)\r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 나는 이미 전처리가 되어있으므로, split으로 나눠주기만 함.\r\n",
    "def go(sent):\r\n",
    "    words = sent.split(\" \")\r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\r\n",
    "# contents에는 리스트 형태의 기사 내용이 들어가면 된다. 각 인덱스에 한 기사내용이 들어간다.\r\n",
    "# 한줄짜리 contents도 가능\r\n",
    "def text_rank(contents):\r\n",
    "    textrank_output = []\r\n",
    "    all_out = []\r\n",
    "    keyword_extractor = KeywordSummarizer(\r\n",
    "        # tokenize= komoran_tokenize,   # 전처리가 안되어 있을경우\r\n",
    "        tokenize= go,                   # 전처리가 되어있을경우\r\n",
    "        window= -1,\r\n",
    "        verbose =False\r\n",
    "    )\r\n",
    "    # 전처리 : contents의 수만큼 전처리 한 후, textrank_output에 저장\r\n",
    "    for idx in range(len(contents)):\r\n",
    "        word = contents[idx].replace('\\n', '').replace('  ', '')\r\n",
    "        result = re.sub('[(-=.\\'·ㆍ>▷▶◆…‘’“”\\\"#/?:$})]', '', word)\r\n",
    "        textrank_output.append(str(result))\r\n",
    "    for idx in range(len(textrank_output)):   \r\n",
    "        outputs = []\r\n",
    "        try:\r\n",
    "            sents = textrank_output[idx]\r\n",
    "            keywords = keyword_extractor.summarize([sents], topk=10)        # summarize 에는 sents가 list의 형태로 들어가기 때문에, []를 씌워준다.    \r\n",
    "            for word, rank in keywords:\r\n",
    "                word = word.split(\"/\")[0]                           # 단어만 추출(단어/형태소분류)\r\n",
    "                outputs.append('{}/{:.3}'.format(word, rank))       # 점수를 3자리까지만 표시\r\n",
    "                # outputs.append('{}'.format(word))\r\n",
    "            \r\n",
    "            outputs = \" \".join(outputs)\r\n",
    "            all_out.append(outputs)                                 # all_out에 저장\r\n",
    "        except:\r\n",
    "            print(\"except\")\r\n",
    "            continue\r\n",
    "        \r\n",
    "    return all_out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 각 cluster에서 5위 안에 드는 단어 뽑기\r\n",
    "\r\n",
    "텍스트랭크로 출력된 결과를 모두 더해 해당 단어의 갯수만큼 나눠서 내림차순으로 정렬."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 위에서 분류된 파일을 불러온다.\r\n",
    "import pandas as pd\r\n",
    "clu_df = pd.read_csv(\"./clustering_etoday.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# value_counts() : 해당 column 에 속한 원소 각각의 갯수를 출력함\r\n",
    "clu_df['cluster'].value_counts()[:20]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clu_df['cluster'].value_counts()[20:50]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# clustering 결과로 CNo 랑 Subject 출력하기.\r\n",
    "cluster_list = []\r\n",
    "for line in range(len(clu_df)):\r\n",
    "    if clu_df[\"cluster\"][line] == 5041:\r\n",
    "        print(clu_df[\"CNo\"][line], clu_df[\"Subjects\"][line])\r\n",
    "        cluster_list.append(clu_df[\"Contents\"][line])\r\n",
    "        \r\n",
    "score_sum = {}      # 단어 스코어 합산 측정\r\n",
    "count = {}          # 단어 갯수 측정\r\n",
    "for news in text_rank(cluster_list):\r\n",
    "    word_score = news.split(\" \")\r\n",
    "    \r\n",
    "    for i in word_score:\r\n",
    "        word, score = i.split(\"/\")\r\n",
    "        try: \r\n",
    "            score_sum[word] += float(score)\r\n",
    "            count[word] += 1\r\n",
    "        except: \r\n",
    "            score_sum[word] = float(score)\r\n",
    "            count[word] = 1\r\n",
    "\r\n",
    "result = {}\r\n",
    "for word in score_sum:\r\n",
    "    result[word] = score_sum[word]/count[word]      # 단어 스코어 합산 결과에서 갯수를 나눠줬다.\r\n",
    "pgm_lang_val_reverse = sorted(result.items(), reverse=True, key=lambda item: item[1])       # 점수(items)로 내림차순 정렬\r\n",
    "\r\n",
    "print(pgm_lang_val_reverse[:5])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}