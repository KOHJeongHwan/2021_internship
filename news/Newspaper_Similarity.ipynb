{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이투데이 vs. 연합뉴스 기사 유사도 비교\n",
    "\n",
    "- 연합뉴스 일정기간 기사 원문을 Scikit-learn의 TFIDF Vectorizer로 train(fit)하여 Vocabulary와 IDF를 만들고\n",
    "- Dcoument Term Matrix(DTM)을 계산(transform)\n",
    "- train된 DTM에 이투데이 기사를 transform하여 생성된 DTM을 train DTM과 dot product하여 cosine 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T22:07:01.418625Z",
     "start_time": "2021-03-08T22:07:01.404236Z"
    }
   },
   "outputs": [],
   "source": [
    "result_data_prefix = '이투데이-연합뉴스-유사도_3_1000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab Tokenizer\n",
    "- 기사 원문의 모든 단어/어절을 vector화 할 경우연산이 많아지며, 문맥에 무의미한 불용어(stopword)도 포함되는 문제를 해소하기 위해\n",
    "- Mecab을 사용하여 문장 중 유의미한 품사만 token으로 사용하고, Mecab 사전에 등록된 한자단어가 NNG로 분석되는 특성을 회피하기 위해 '한자 배제' 정규식 적용\n",
    "- 추가로 stopword를 지정하여 불용어 제거 (현재는 Mecab의 분석 결과가 특별한 불용어 처리가 필요하지 않아 비워두었음)\n",
    "- 비교 테스트를 위해 KoNLPy의 Mecab을 사용하였으나, 단독 버전 MeCab으로 사용하는 것이 메모리 소요나 호출 시간에 유리하지 않을지 고민 중..."
   ]
  },
  {
   "source": [
    "### 정환 메모\n",
    "\n",
    "바로 아래 코드는 회사에서 만든 토크나이저 파일이 필요하다. 받은 파일이 없다면 그냥 넘어갈 것!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T22:07:03.572130Z",
     "start_time": "2021-03-08T22:07:03.133800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.append('D:/Projects/WIGO/Tokenization')\n",
    "# sys.path.append(os.path.dirname(os.path.abspath('../similarity/tokenization')))\n",
    "from tokenization.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    tagger=\"Mecab\",\n",
    "    pos=[\"NNG\",\"NNP\", \"NP\", \"XPN\", \"XR\", \"VV\", \"VA\",\"VX\", \"VCP\",\"VCN\", \"XSV\", \"XSA\", \"MM\", \"MAG\", \"MAJ\", \"IC\", \"SN\"]\n",
    ")\n",
    "' '.join(tokenizer.tokenize('우리는 민족중흥의 역사적 사명을 띠고 이 땅에 태어났다. 금일 코로나 환자 19명 발생!!!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data 로드\n",
    "\n",
    "- 현재 시나리오에 맞추어 file_name, title, body 로 고정된 CSV인데\n",
    "- 일반화를 위해 CSV 구조를 확장 가능하도록 설계하거나\n",
    "- CSV 로드 부분을 메인로직에서 빼내어 task별로 별도의 모듈로 만드는 방안 수립 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T22:07:09.803255Z",
     "start_time": "2021-03-08T22:07:06.669689Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['합동', '차례', '민속놀이', '영상', '통화', '향수', '주민', '초청', '행사', '생략', '이역만리', '파병', '장병', '코로나', '와중', '추석', '합동', '차례', '민속놀이', '영상', '통화', '향수', '주민', '초청', '행사', '생략', '유현민', '기자', '이역만리', '구슬땀', '천', '해외', '파병', '장병', '민족', '최대', '명절', '한가위', '합동', '참모', '본부', '군', '운용', '해외', '파병', '부대', '장병', '추석', '합동', '차례', '가족', '영상', '통화', '고국', '그리움', '올해', '신종', '코로나', '바이러스', '감염증', '예방', '예년', '외부', '활동', '현지', '주민', '초청', '행사', '레바논', '유엔', '평화', '유지', '활동', '동명', '부대', '감시', '정찰', '임무', '평소', '수행', '합동', '차례', '대형', '윷놀이', '투호', '민속놀이', '가족', '아쉬움', '아프리카', '남수단', '재건', '지원', '활동', '한빛', '부대', '합동', '차례', '윷놀이', '제기차기', '명절', '분위기', '소말리아', '아덴만', '해역', '다국적군', '상선', '보호', '임무', '수행', '청해부대', '군수품', '보급', '오만', '기항', '추석', '부대', '다음', '항해', '준비', '합동', '차례', '민속놀이', '영상', '통화', '프로그램', '마련', '아랍에미리트', '에', '파병', '아크', '부대', '장병', '최근', '군', '연합', '훈련', '성공', '고국', '가족', '영상', '통화', '추석', '아크', '부대장', '박용규', '중령', '코로나', '제한', '상황', '부여', '임무', '성공', '완수', '장병', '사기', '가족', '해외', '우리', '국군', '위상', '최선', '고', '말'], ['연구', '팀', '회전력', '용액', '밀도', '순서', '분리', '계면', '화학', '반응', '단계', '진행', '진통제', '의약', '화합물', '합성', '활용', '입증', '네이처', '에', '논문', '발표', '사이', '테크', '플러스', '원통형', '용기', '단계', '연쇄', '화학합성', '제어', '시스템', '개발', '연구', '팀', '회전력', '용액', '밀도', '순서', '분리', '계면', '화학', '반응', '단계', '진행', '진통제', '의약', '화합물', '합성', '활용', '입증', '네이처', '에', '논문', '발표', '이주영', '기자', '국내', '연구', '의약', '물질', '화합물', '합성', '필요', '여러', '단계', '화학', '반응', '원통형', '용기', '하나', '정밀', '제어', '진행', '화학합성', '플랫', '폼', '개발', '기초', '과학', '연구원', '첨단', '연성', '물질', '연구', '단', '바르', '토슈', '그', '쥐', '보프', '스키', '그룹', '리더', '연구', '팀', '국제', '학술지', '네이처', '원통형', '반응', '용기', '하나', '여러', '화학', '공정', '처리', '화학', '합성', '시스템', '개발', '의약', '물질', '복잡', '화합물', '단순', '분자', '여러', '단계', '화학', '반응', '합성', '이런', '물질', '합성', '화학', '반응', '단계', '반응', '용기', '화학', '반응', '원료', '용액', '관', '밸브', '이용', '반응', '제어', '방법', '사용', '이런', '방법', '자동', '장치', '제작', '반응물', '흐름', '조절', '고도', '공학', '기술', '필요', '한계점', '구진', '물', '기름', '밀도', '다른', '용액', '분리', '착안', '원통형', '용기', '안', '밀도', '다른', '용액', '회전력', '이용', '바깥', '밀도', '용액', '층', '시스템', '고안', '회전', '원통', '원심력', '밀도', '액체', '바깥쪽', '원리', '이용', '화학', '반응', '필요', '용액', '밀도', '차이', '차례', '이동', '분리', '원', '화학', '반응', '안쪽', '바깥쪽', '순차', '진행', '조절', '구진', '설명', '연구', '이', '시스템', '용액', '층', '구현', '결과', '원통형', '용기', '안', '밀도', '다른', '용액', '층', '가능', '각', '용액', '층', '두께', '까지', '확인', '구진', '실제', '의약', '물질', '합성', '화학', '산업', '원료', '물질', '화합물', '추출', '실험', '이', '시스템', '화학', '실험', '중소', '규모', '화학', '반응', '활용', '입증', '이', '회전식', '원통형', '용기', '안', '진통제', '페나세틴', '항', '아메바', '약물', '딜', '록', '사니', '드', '단계', '합성', '아미노산', '하나', '감미료', '아스파탐', '주재료', '페닐알라닌', '화합물', '추출', '성공', '구진', '이', '시스템', '기존', '화학합성', '과정', '단순', '화학', '산업', '희귀', '금속', '추출', '다양', '화합물', '합성', '시간', '비용', '아이디어', '제시', '실제', '응용', '의의', '설명', '공동', '저자', '올', '게르', '스키', '연구', '위원', '이번', '개발', '시스템', '합성', '과정', '영향', '핵심', '변수', '자유자재', '조절', '용매', '층', '사이', '작용', '조절', '기존', '추출', '화합물', '추출', '활용', '무궁무진', '고', '말'], ['연합뉴스', '콘텐츠', '저작', '고지', '연합', '뉴스', '제공', '기사', '사진', '그래픽', '영상', '모든', '콘텐츠', '관련', '법', '보호', '연합뉴스', '콘텐츠', '사전', '허가', '전재', '방송', '무단', '복사', '배포', '판매', '전시', '개작', '경우', '민', '형', '사상', '책임', '콘텐츠', '사용', '관련', '궁금', '점', '전화', '이메일', '문의'], ['아제르바이잔', '아르메니아', '르노', '카라바흐', '서', '나흘', '교전', '아', '제르', '아르메니아', '철수', '아르메니아', '평화', '협상', '시기상조', '터키', '아제르바이잔', '요청', '필요', '지원', '군사', '지원', '시사', '아제르바이잔', '아르메니아', '르노', '카라바흐', '서', '나흘', '교전', '아', '제르', '아르메니아', '철수', '아르메니아', '평화', '협상', '시기상조', '유철종', '특파원', '남캅카스', '분쟁', '지역인', '르노', '카라바흐', '앙숙', '관계', '옛', '소련', '국가', '아제르바이잔', '아르메니아', '나흘', '교전', '가운데', '터키', '아제르바이잔', '군사', '지원', '입장', '터키', '튀르크', '국가', '아제르바이잔', '군사', '경제', '지원', '튀르크', '어', '사용', '터키', '아제르바이잔', '국민', '의사소통', '가능', '서로', '형제', '국가', '로이터', '통신', '뤼트', '차우', '쇼', '루', '터키', '외무', '장관', '이날', '자국', '관영', '아나돌루', '통신', '인터뷰', '아제르바이잔', '요청', '터키', '군사', '지원', '제공', '질문', '요청', '필요', '군사', '지원', '가능', '시사', '아르메니아', '르노', '카라바흐', '지역', '개전', '이후', '터키', '시리아', '용병', '아제르바이잔', '전투', '투입', '주장', '레제프', '타이', '이프', '에르', '터키', '대통령', '터키', '모든', '자원', '마음', '아제르바이잔', '지원', '말', '이날', '차우', '쇼', '루', '장관', '발언', '아제르바이잔', '아르메니아', '전투', '격화', '경우', '터키', '아제르바이잔', '본격', '군사', '지원', '제공', '시사', '풀이', '일함', '알리예프', '아제르바이잔', '대통령', '이날', '전투', '부상자', '면담', '아르메니아', '조건', '우리', '땅', '아르메니아', '정부', '이', '조건', '이행', '전투', '중단', '피', '그', '나흘', '지속', '전투', '역사', '정의', '복원', '르노', '카라바흐', '아제르바이잔', '역사', '영토', '강조', '반면', '니콜', '파쉬', '아르메니아', '총리', '이날', '러시아', '중재', '아제르바이잔', '평화', '협상', '시기상조', '일축', '총리', '러시아', '인테르', '팍스', '통신', '인터뷰', '심각', '행위', '시점', '아르메니아', '아제르바이잔', '러시아', '정상', '회담', '얘기', '부적절', '고', '지적', '블라디미르', '푸틴', '러시아', '대통령', '전날', '파쉬', '총리', '통화', '르노', '카라바흐', '상황', '논의', '해당', '지역', '전투', '행위', '계속', '우려', '표시', '푸틴', '아르메니아', '아제르바이잔', '전투', '중단', '위기', '해소', '조치', '필요', '조언', '파쉬', '하산', '로', '이란', '대통령', '전화', '통화', '르노', '카라바흐', '사태', '논의', '아르메니아', '총리', '실', '총리', '로', '대통령', '터키', '군사', '행동', '참여', '이', '로', '대통령', '아르메니아', '아제르바이잔', '군사', '긴장', '우려', '표시', '총리', '소개', '아제르바이잔', '아르메니아', '국제', '사회', '자제', '호소', '무시', '르노', '카라바흐', '분쟁', '지역', '이후', '최대', '규모', '교전', '나흘', '계속', '아르메니아', '이날', '르노', '카라바흐', '지역', '도시', '마르타', '케르', '트', '아제르바이잔', '공격', '민간인', '사망', '발표', '아르메니아', '정부', '자체', '웹', '사이트', '터키', '전투기', '전날', '격추', '주장', '자국', '수호이', '전투기', '잔해', '사진', '터키', '전투', '개입', '비난', '아제르바이잔', '대통령실', '아르메니아', '주장', '반박', '아르메니아', '전투기', '추락', '산', '충돌', '파괴', '반박', '아제르바이잔', '아르메니아', '아르메니아', '실효', '지배', '옛', '아제르바이잔', '영토', '르노', '카라바흐', '지역', '교전'], ['이탈리아', '방문', '미', '폼페이', '이탈리아', '중국', '경제', '밀착', '전성훈', '특파원', '이탈리아', '방문', '마이크', '폼페이', '미국', '국무', '장관', '이탈리아', '중국', '경제', '협력', '진전', '우려', '뜻', '전달', '현지', '언론', '로이터', '통신', '폼페이', '오', '장관', '루이지', '디', '마이오', '이탈리아', '외무', '장관', '회담', '뒤', '공동', '기자', '회견', '우리', '중국', '전략', '목적', '이탈리아', '경제', '영향력', '확대', '시도', '관련', '미국', '우려', '화두', '장시간', '대화', '고', '말', '그', '미국', '정부', '중국', '기술', '기업', '야기', '국가', '안보', '개인', '정보', '관련', '위협', '이탈리아', '정부', '신중', '고려', '촉구', '강조', '특정', '기업', '언급', '세대', '이동', '통신', '구축', '사업', '중국', '통신', '장비', '업체', '화웨이', '배제', '우회', '강조', '풀이', '이', '디', '마이오', '장관', '미국', '입장', '이해', '중국', '관련', '이슈', '미국', '비롯', '핵심', '우방', '보조', '의지', '피력', '이탈리아', '주요', '서방', '국가', '가운데', '유일', '중국', '글로벌', '확장', '정책', '일대일', '에', '참여', '중국', '밀접', '경제', '외교', '관계', '유지', '최근', '이탈리아', '최대', '통신', '업체', '텔레콤', '이탈리아', '의', '구축', '사업', '입찰', '화웨이', '참여', '일부', '변화', '조짐', '감지', '일각', '지속', '미국', '압력', '유럽연합', '도', '화웨이', '부정', '시각', '입찰', '배제', '관측', '이탈리아', '신종', '코로나', '바이러스', '감염증', '막대', '경제', '타격', '회원국', '도입', '회복', '기금', '최대', '수혜자', '원만', '관계', '핵심', '이슈', '부각', '상황']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# train_data_path = 'D:/Projects/_Corpus/DataBoucher_20201110/yeonhap.csv'\n",
    "train_data_path = './pretreatment_data/yeonhap_pre.csv'\n",
    "# 학습용 데이터를 로드하고 DataFrame 생성\n",
    "train_df = pd.read_csv(train_data_path, encoding='utf-8', names=['CNo', 'Subject', 'Contents'])\n",
    "#train_df.head()\n",
    "#print(train_df.info())\n",
    "test = [df.split(' ') for df in train_df['Contents']]\n",
    "\n",
    "print(test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer 생성\n",
    "\n",
    "- 한글의 경우 lowercase는 False로 설정하는 것이 좋음 (다른 Task에서의 경험, 일부 깨지는 현상이 있을 수 있음)\n",
    "- ngram은 1,2 등 다른 값을 주어 테스트 해 보았으나, 형태소 분석을 거친 token의 경우 1,1이 가장 성능(실행시간, 유사도 모두)이 좋음\n",
    "- max_feature는 50,000으로 지정하였으나, train set 저장을 고려할 때 Database 전체를 하나의 train set으로 만들 수 있는지 실험 필요(아마도 어려울 것)\n",
    "- preprocessor는 좋은 개념으로 보여 일단 선언만 해 두었으며, 이후 추가 study 필요\n",
    "- vocabulary는 미리 생성하여 지정하면, fit 시간이 줄어드는지 여부 확인 필요"
   ]
  },
  {
   "source": [
    "### 정환 메모\n",
    "\n",
    "TfidfVectorizer 옵션을 설정하는 코드\n",
    "\n",
    "따로 토크나이저 파일을 받지 않았다면 tokenizer=, token_patten=, vocabulary= 이 3가지 옵션은 주석처리 하자!\n",
    "\n",
    "궁금증 => 외부 토크나이저는 어떻게 사용할까?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T22:07:12.534516Z",
     "start_time": "2021-03-08T22:07:12.517515Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=False, # 소문자로 바꿔줌, 한글에서는 사용 X (default = True)\n",
    "    preprocessor=None, # \n",
    "    # tokenizer=tokenizer,\n",
    "    # tokenizer=[df.split(' ') for df in train_df['Contents']],\n",
    "    analyzer = 'word', # 학습단위를 'word' 또는 'char'로 지정\n",
    "    # stop_words=tokenizer.stopword,\n",
    "    # token_pattern=None, # tokenizer가 선언되면 token_pattern은 None처리할 것\n",
    "    ngram_range = (1, 1), # 1단어부터 n단어까지 묶음으로 Vocabulary 생성\n",
    "    #max_df = 0.80 #: 문서의 80% 이상에 나타나는 단어 무시\n",
    "    # max_df = 10 : 10개 이상의 문서에 나타나는 단어 무시\n",
    "    max_df=1000,\n",
    "    #min_df = 0.01 #: 문서의 1% 미만으로 나타나는 단어 무시\n",
    "    # min_df = 10 : 문서 10개 미만에 나타나는 단어 무시\n",
    "    min_df=3, # 토큰이 나타날 최소 문서 개수로 오타나 자주 나오지 않는 특수한 전문용어 제거\n",
    "    max_features = 50000, # Features의 갯수를 제한\n",
    "    # vocabulary=None,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True, # tf값에 1+log(tf)를 적용하여 tf값이 무한정 커지는 것을 막음\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- 실험 조건 51,864개 기사 69.8MB 학습 시간이 fit 30-36초 transform 32-38초 소요\n",
    "- 학습 후 Pickle 또는 Joblib로 저장하는 방안 고려 필요\n",
    "- 저장을 고려할 때 fit 한 상태로 저장할 것인지, transform 한 상태로 저장할 것인지 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-08T22:07:15.383Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64122, 34548)\n"
     ]
    }
   ],
   "source": [
    "# training set으로부터 vocabulary와 idf를 학습하고 Document Term Matrix(이하 DTM)을 리턴\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['Contents'])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump to File\n",
    "\n",
    "- joblib를 이용하여 압축률 3으로 Prdiction에 필요한 객체(train_df, tfidf_matrix, tfidf_vectorizer)를 저장"
   ]
  },
  {
   "source": [
    "### 정환 메모\n",
    "\n",
    "joblib와 pickle 뭐가 다를까?\n",
    "\n",
    "joblib 가 최신 버전! joblib도 내부를 보면 pickle를 사용함.\n",
    "\n",
    "상황에 따라 사용하자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T04:46:13.724522Z",
     "start_time": "2020-11-10T04:46:07.786662Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compress: 0~9의 int 또는 bool (True이면 3) 또는 2-Tuple\n",
    "# protocol: 버전 3.0에서 기본 프로토콜은 3 (버전 3.8에서 기본 프로토콜은 4)\n",
    "joblib.dump((train_df, tfidf_matrix, tfidf_vectorizer), f'{result_data_prefix}-train.joblib', compress=True, protocol=3)\n",
    "\n",
    "# Memory 해제\n",
    "del train_df\n",
    "del tfidf_matrix\n",
    "del tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from File\n",
    "\n",
    "- Joblib로 저장된 객체를 읽고 Instance화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T04:46:37.857615Z",
     "start_time": "2020-11-10T04:46:35.876616Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import csv\n",
    "import math\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Queue\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "alloted_CPUs = multiprocessing.cpu_count() * 0.5\n",
    "num_cores = int(alloted_CPUs + alloted_CPUs % 2)\n",
    "\n",
    "train_df, tfidf_matrix, tfidf_vectorizer = joblib.load(f'{result_data_prefix}-train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity 연산 및 결과 저장\n",
    "\n",
    "- 현재 시나리오에서는 학습과 적용, 저장이 분리될 필요가 없어 운용에 편의를 위해 하나의 파일에 구현하였으나\n",
    "- 일반화를 고려할 때 별도의 API에 구현되어야 할 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T04:46:40.112251Z",
     "start_time": "2020-11-10T04:46:40.103253Z"
    }
   },
   "outputs": [],
   "source": [
    "def CalcSimilarity(tfidf_matrix, train_df, tfidf_vectorizer, q):\n",
    "    target_matrix = tfidf_vectorizer.transform([target[2].replace('ㆍ','*')])\n",
    "\n",
    "    # dot product 후 crs_matrix를 nparray로 변환하고 차원을 축소 1차원 array로 변환\n",
    "    scores = (tfidf_matrix * target_matrix.T).toarray().sum(axis=1)\n",
    "\n",
    "    for j in scores.argsort()[::-1]:\n",
    "        if scores[j] >= interval_min and scores[j] < interval_max: # min 이상 max 미만\n",
    "            itemDict = {'targetFilename':'', 'trainFilename':'', 'score':''}\n",
    "            itemDict['targetFilename'] = target[0]\n",
    "            itemDict['trainFilename'] = train_df._get_value(j, 'file_name')\n",
    "            itemDict['score'] = math.floor(scores[j] * 1000) / 1000\n",
    "            q.put(itemDict)\n",
    "    q.put('STOP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Dataframe으로 for loop을 돌며 cosine similarity 연산하고 row 별로 직접 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "print(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:02:58.488506Z",
     "start_time": "2020-11-10T06:56:16.750507Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "target_df loop:   0%|          | 0/9747 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3ee8afd89b14b14a6ed1ca2b5284bfc"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# target_data_path = 'D:/Projects/_Corpus/DataBoucher_20201110/etoday.csv'\n",
    "target_data_path = './pretreatment_data\\etoday_pre.csv'\n",
    "\n",
    "# Target도 같은 tfidf_vectorizer로 transform하고 dot product\n",
    "target_df = pd.read_csv(target_data_path, encoding='utf-8', names=['CNo', 'Subject', 'Contents'])\n",
    "\n",
    "# 유사도 산출 구간을 min =< ~ > max로 지정\n",
    "# 실험결과 1.0인 경우 Python의 특성상 소수점 아래 가비지가 발생하여 2.0으로 비교할 것\n",
    "interval_min = 0.7\n",
    "interval_max = 0.8\n",
    "\n",
    "# Data의 양이 많으므로 DataFrame을 만들지 않고 직접 저장\n",
    "with open(f'{result_data_prefix}-{interval_min}_{interval_max}.csv', 'wt', encoding='utf-8', newline='') as file_csv:\n",
    "    writer = csv.writer(file_csv, delimiter=',', lineterminator='\\n') # 콤마(,)를 delimiter로 사용\n",
    "    for i in tqdm(target_df.index, desc='target_df loop: ', position=0):\n",
    "        # train vectorizer에 target을 transform\n",
    "        target_matrix = tfidf_vectorizer.transform([target_df._get_value(i, 'Contents')]).astype(np.float16)\n",
    "\n",
    "        # dot product 후 crs_matrix를 nparray로 변환하고 차원을 축소 1차원 array로 변환 (cosine_similarity 보다 다소 빠르고 score는 다소 작게 계산됨)\n",
    "        scores = (tfidf_matrix * target_matrix.T).toarray().reshape(-1,)\n",
    "        # cosine_similarity 연산 후 nparray 차원을 축소 1차원 array로 변환\n",
    "        # scores = cosine_similarity(tfidf_matrix, target_matrix).reshape(-1,)\n",
    "\n",
    "        for j in scores.argsort()[::-1]: # 내림차순 정렬\n",
    "            score = math.floor(scores[j] * 10000) / 10000 # 소수점 두자리 이하 버림\n",
    "            if score >= interval_min and \\\n",
    "                ((interval_max < 1.0 and score < interval_max) or (interval_max == 1.0 and score <= interval_max)): # min 이상 max 미만, max가 1일 경우는 max 이하\n",
    "                #print('{} and {} / score : {}'.format(target_df._get_value(i, 'CNo'), train_df._get_value(j, 'CNo'), score))\n",
    "                writer.writerow([target_df._get_value(i, 'CNo'), train_df._get_value(j, 'CNo'), score]) # 하나의 row를 리스트 형태로 입력\n",
    "            elif interval_min > score:\n",
    "                break\n",
    "\n",
    "# Memory 해제\n",
    "#del target_df\n",
    "#del train_df\n",
    "#del tfidf_matrix\n",
    "#del tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 소장님이 개인적으로 이것저것 실험해 본 코드라서 볼 필욘 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:41:02.242161Z",
     "start_time": "2020-11-10T07:41:01.504590Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#del df1, df2, df3, df4\n",
    "numOfRows = 1000\n",
    "\"\"\"\n",
    "# append\n",
    "startTime = time.perf_counter()\n",
    "df1 = pd.DataFrame(np.random.randint(100, size=(5,5)), columns=['A', 'B', 'C', 'D', 'E'])\n",
    "for i in range( 1,numOfRows-4):\n",
    "    df1 = df1.append( dict( (a,np.random.randint(100)) for a in ['A','B','C','D','E']), ignore_index=True)\n",
    "print('Elapsed time: {:6.3f} seconds for {:d} rows'.format(time.perf_counter() - startTime, numOfRows))\n",
    "print(df1.shape)\n",
    "\n",
    "# .loc w/o prealloc\n",
    "startTime = time.perf_counter()\n",
    "df2 = pd.DataFrame(np.random.randint(100, size=(5,5)), columns=['A', 'B', 'C', 'D', 'E'])\n",
    "for i in range( 1,numOfRows):\n",
    "    df2.loc[i]  = np.random.randint(100, size=(1,5))[0]\n",
    "print('Elapsed time: {:6.3f} seconds for {:d} rows'.format(time.perf_counter() - startTime, numOfRows))\n",
    "print(df2.shape)\n",
    "\n",
    "# .loc with prealloc\n",
    "df3 = pd.DataFrame(index=np.arange(0, numOfRows), columns=['A', 'B', 'C', 'D', 'E'] )\n",
    "startTime = time.perf_counter()\n",
    "for i in range( 1,numOfRows):\n",
    "    df3.loc[i]  = np.random.randint(100, size=(1,5))[0]\n",
    "print('Elapsed time: {:6.3f} seconds for {:d} rows'.format(time.perf_counter() - startTime, numOfRows))\n",
    "print(df3.shape)\n",
    "\"\"\"\n",
    "# dict\n",
    "startTime = time.perf_counter()\n",
    "row_list = []\n",
    "#for i in range (0,5):\n",
    "#    row_list.append(dict((a,np.random.randint(100)) for a in ['A','B','C','D','E']))\n",
    "\n",
    "for i in range(0, numOfRows):\n",
    "    dict1 = dict((a,np.random.randint(100)) for a in ['A','B','C','D','E'])\n",
    "    row_list.append(dict1)\n",
    "\n",
    "df4 = pd.DataFrame(row_list, columns=['A','B','C','D','E'])\n",
    "print('Elapsed time: {:6.3f} seconds for {:d} rows'.format(time.perf_counter() - startTime, numOfRows))\n",
    "print(df4.shape)\n",
    "#print(row_list)\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:41:41.596502Z",
     "start_time": "2020-11-10T07:41:41.593501Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T07:41:42.178938Z",
     "start_time": "2020-11-10T07:41:42.156935Z"
    }
   },
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, 4)\n",
    "    pool = Pool(4)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def add_features(df):\n",
    "    df['question_text'] = df['question_text'].apply(lambda x:str(x))\n",
    "    df[\"lower_question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    df['total_length'] = df['question_text'].apply(len)\n",
    "    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.question_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words'] \n",
    "    df['num_exclamation_marks'] = df['question_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['question_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['num_punctuation'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['num_symbols'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "    df['num_smilies'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    df['num_sad'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in (':-<', ':()', ';-()', ';(')))\n",
    "    df[\"mean_word_len\"] = df[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T14:30:10.802698Z",
     "start_time": "2020-09-10T14:30:10.622617Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"D:/Projects/_Corpus/DataBoucher/etoday.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-10T14:30:12.791Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "train = parallelize_dataframe(train_df, add_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "train = add_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T11:30:39.166927Z",
     "start_time": "2020-09-23T11:30:39.149883Z"
    }
   },
   "outputs": [],
   "source": [
    "SELECT_ARTICLE_PROCEDURE = '''\n",
    "    EXEC EYE_COMMON.dbo.ArticleSimilarity_sel @startdate = '%s', @enddate = '%s', @sct = '%s'\n",
    "'''\n",
    "\n",
    "print(SELECT_ARTICLE_PROCEDURE % ('2020-01-01', '2020-12-31', 'AA001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('news': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "72dd90e7b1ce7a48b28470bb78287cf3415c6e0697c2ed54321a33a92b38e591"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}