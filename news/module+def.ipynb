{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('news': conda)"
  },
  "interpreter": {
   "hash": "72dd90e7b1ce7a48b28470bb78287cf3415c6e0697c2ed54321a33a92b38e591"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 실험적인 코드\n",
    "\n",
    "TF-IDF 를 직접 구현도 해보고, 모듈도 써본 테스트 코드\n",
    "\n",
    "완성본은 main 이나 main(최종)를 확인 바람.\n",
    "\n",
    "직접 구현한 코드는 연산시 메모리 부족 문제로 2만개씩만 계산함! 그래서 대부분 3등분 되어있다.\n",
    "\n",
    "모듈은 6만개 이상 연산해도 메모리가 널널하다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 64122 entries, 0 to 64121\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   CNo       64122 non-null  object\n 1   Subject   64122 non-null  object\n 2   Contents  64122 non-null  object\ndtypes: object(3)\nmemory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./pretreatment_data/yeonhap_pre.csv', names=['CNo', 'Subject', 'Contents'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9747 entries, 0 to 9746\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   CNo       9747 non-null   object\n 1   Subject   9747 non-null   object\n 2   Contents  9747 non-null   object\ndtypes: object(3)\nmemory usage: 228.6+ KB\n"
     ]
    }
   ],
   "source": [
    "et_df = pd.read_csv('./pretreatment_data/etoday_pre.csv', names=['CNo', 'Subject', 'Contents'])\n",
    "et_df.info()"
   ]
  },
  {
   "source": [
    "## 단어 리스트 뽑아내기\n",
    "\n",
    "커널을 다시 시작할 때 마다 token_list_ 는 만들어 줘야 한다.\n",
    "\n",
    "2~3분 밖에 안 걸리니깐 기다리자."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# 작업 시간 2~3분 정도 걸림\n",
    "# 모든 문서에서 사용된 단어들을 뽑아내는 작업\n",
    "temp = df['Contents'][0]\n",
    "token_list_all = []\n",
    "cnt = 0\n",
    "for doc in df['Contents'][1:]:\n",
    "    temp = temp + \" \" + doc\n",
    "    if cnt % 1000 == 0:\n",
    "        print(cnt, \"완료\")\n",
    "    cnt+=1\n",
    "token_list_all = temp.split(\" \")\n",
    "token_list_all = list(set(token_list_all))\n",
    "token_list_all = token_list_all[1:]\n",
    "print(token_list_all)\n",
    "print(len(token_list_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token_list_all.pickle', 'wb') as handle:\n",
    "    pickle.dump(token_list_all, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"idf_dit_1.pickle\", 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# 작업 시간 2~3분 정도 걸림\n",
    "# 20000개 문서에서 사용된 단어들을 뽑아내는 작업\n",
    "temp = \"\"\n",
    "token_list_1 = []\n",
    "for doc in df['Contents'][:20000]:\n",
    "    temp = temp + \" \" + doc\n",
    "token_list_1 = temp.split(\" \")\n",
    "token_list_1 = list(set(token_list_1))\n",
    "token_list_1 = token_list_1[1:]\n",
    "print(token_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "temp = \"\"\n",
    "token_list_2 = []\n",
    "for doc in df['Contents'][20000:40000]:\n",
    "    temp = temp + \" \" + doc\n",
    "token_list_2 = temp.split(\" \")\n",
    "token_list_2 = list(set(token_list_2))\n",
    "token_list_2 = token_list_2[1:]\n",
    "print(token_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "temp = \"\"\n",
    "token_list_3 = []\n",
    "for doc in df['Contents'][40000:]:\n",
    "    temp = temp + \" \" + doc\n",
    "token_list_3 = temp.split(\" \")\n",
    "token_list_3 = list(set(token_list_3))\n",
    "token_list_3 = token_list_3[1:]\n",
    "print(token_list_3)"
   ]
  },
  {
   "source": [
    "## 실행에 필요한 함수들"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, document):\n",
    "    # document 에서 term 의 등장 횟수를 count\n",
    "    return document.count(term)\n",
    "\n",
    "def idf(term, documents):\n",
    "    # doc_list 에서 term 이 등장한 문서 수를 count\n",
    "    \n",
    "    # 함수가 돌 때마다 초기화\n",
    "    dfn = 0\n",
    "    for doc in documents:\n",
    "        # 각 문서마다 해당 언어가 있는지 확인\n",
    "        if term in doc:\n",
    "            dfn = dfn + 1\n",
    "    return log(len(documents)/(dfn+1))\n",
    "    \n",
    "def tfidf(term, docu):\n",
    "    # tf * idf\n",
    "    return tf(term, docu)*idf(term)"
   ]
  },
  {
   "source": [
    "## 단어 카운트 리스트 만들기\n",
    "\n",
    "IDF 딕션어리를 만들때 dtm (단어 카운트 리스트) 가 필요.\n",
    "\n",
    "idf_idt pickle이 있다면 넘어가도 된다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "dtm_1 = []\n",
    "cnt = 0\n",
    "for doc in df['Contents'][:20000]:\n",
    "    # document term matrix (문서별 단어 등장 횟수) 를 구현해보자, 4 (문서) x 433 (단어) 의 행렬을 리스트로 구성하면 된다\n",
    "\n",
    "    # 방금 추가한 문서의 리스트\n",
    "    if cnt % 10 == 0:\n",
    "        print(\"{} page 완료\".format(cnt))\n",
    "    dtm_1.append([])\n",
    "    # docnment = doc.split(\" \") -> split 안해도 ㄱㅊ. 안에 있는지 없는지 찾아 낼 수 있음.\n",
    "    for token in token_list_1:\n",
    "        # -1은 마지막 인덱스\n",
    "        dtm_1[-1].append(tf(token, doc))\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_2 = []\n",
    "cnt = 0\n",
    "for doc in df['Contents'][20000:40000]:\n",
    "    # document term matrix (문서별 단어 등장 횟수) 를 구현해보자, 4 (문서) x 433 (단어) 의 행렬을 리스트로 구성하면 된다\n",
    "\n",
    "    # 방금 추가한 문서의 리스트\n",
    "    if cnt % 10 == 0:\n",
    "        print(\"{} page 완료\".format(cnt))\n",
    "    dtm_2.append([])\n",
    "    for token in token_list_2:\n",
    "        # -1은 마지막 인덱스\n",
    "        dtm_2[-1].append(tf(token, doc))\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_3 = []\n",
    "cnt = 0\n",
    "for doc in df['Contents'][40000:]:\n",
    "    # document term matrix (문서별 단어 등장 횟수) 를 구현해보자, 4 (문서) x 433 (단어) 의 행렬을 리스트로 구성하면 된다\n",
    "\n",
    "    # 방금 추가한 문서의 리스트\n",
    "    if cnt % 10 == 0:\n",
    "        print(\"{} page 완료\".format(cnt))\n",
    "    dtm_3.append([])\n",
    "    for token in token_list_3:\n",
    "        # -1은 마지막 인덱스\n",
    "        dtm_3[-1].append(tf(token, doc))\n",
    "    cnt += 1"
   ]
  },
  {
   "source": [
    "pandas로 변환해서 봐도 되는데, 용량이 워낙 큰 바람에 메모리 부족"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_pd = pd.DataFrame(dtm, columns=token_list)\n",
    "print(dtm_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm_pd.iloc[1])"
   ]
  },
  {
   "source": [
    "## IDF 딕션어리 만들기\n",
    "\n",
    "token list 가 필요하다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전부\n",
    "# 그룹 0\n",
    "idf_dit_all = {}\n",
    "cnt = 0\n",
    "for token in token_list_all:\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} 완료\".format(cnt))\n",
    "    idf_dit_all[token] = idf(token, df['Contents'])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2만까지\n",
    "# 그룹 1\n",
    "idf_dit_1 = {}\n",
    "cnt = 0\n",
    "for token in token_list_1:\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} 완료\".format(cnt))\n",
    "    idf_dit_1[token] = idf(token, df['Contents'][:20000])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4만까지\n",
    "# 그룹 2\n",
    "idf_dit_2 = {}\n",
    "cnt = 0\n",
    "for token in token_list_2:\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} 완료\".format(cnt))\n",
    "    idf_dit_2[token] = idf(token, df['Contents'][20000:40000])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 끝까지\n",
    "# 그룹 3\n",
    "idf_dit_3 = {}\n",
    "cnt = 0\n",
    "for token in token_list_3:\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} 완료\".format(cnt))\n",
    "    idf_dit_3[token] = idf(token, df['Contents'][40000:])\n",
    "    cnt+=1"
   ]
  },
  {
   "source": [
    "pickle로 저장"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('idf_dit_all.pickle', 'wb') as handle:\n",
    "    pickle.dump(idf_dit_all, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idf_dit_1 == b)"
   ]
  },
  {
   "source": [
    "## TF-IDF 리스트 만들기\n",
    "\n",
    "위에서 만든 idf 딕셔너리가 필요하다\n",
    "\n",
    "=====>>> 모듈로 대체한다!!!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64122, 65825)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(df['Contents'])\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2만개씩\n",
    "with open(\"idf_dit_1.pickle\", 'rb') as handle:\n",
    "    dit_1 = pickle.load(handle)\n",
    "\n",
    "tfidf_list_1 = []\n",
    "cnt = 0\n",
    "for doc in df['Contents'][:20000]:\n",
    "    tfidf_list_1.append([])\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} 완료\".format(cnt))\n",
    "    for token in token_list_1:\n",
    "        tfidf_list_1[-1].append(tf(token, doc)*dit_1[token])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1만개씩\n",
    "with open(\"idf_dit_1.pickle\", 'rb') as handle:\n",
    "    dit_1 = pickle.load(handle)\n",
    "\n",
    "tfidf_list_1_1 = []\n",
    "cnt = 0\n",
    "for doc in df['Contents'][:10000]:\n",
    "    tfidf_list_1_1.append([])\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} 완료\".format(cnt))\n",
    "    for token in token_list_1:\n",
    "        tfidf_list_1_1[-1].append(tf(token, doc)*dit_1[token])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5000개씩\n",
    "with open(\"idf_dit_1.pickle\", 'rb') as handle:\n",
    "    dit_1 = pickle.load(handle)\n",
    "\n",
    "tfidf_list_1_1_1 = []\n",
    "cnt = 0\n",
    "for doc in df['Contents'][:5000]:\n",
    "    tfidf_list_1_1_1.append([])\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} 완료\".format(cnt))\n",
    "    for token in token_list_1:\n",
    "        tfidf_list_1_1_1[-1].append(tf(token, doc)*dit_1[token])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000개씩\n",
    "with open(\"idf_dit_1.pickle\", 'rb') as handle:\n",
    "    dit_1 = pickle.load(handle)\n",
    "\n",
    "test_1000 = []\n",
    "cnt = 0\n",
    "for doc in df['Contents'][:1000]:\n",
    "    test_1000.append([])\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} 완료\".format(cnt))\n",
    "    for token in token_list_1:\n",
    "        test_1000[-1].append(tf(token, doc)*dit_1[token])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_list_1_1_1.pickle', 'wb') as handle:\n",
    "    pickle.dump(tfidf_list_1_1_1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"tfidf_list_1_1_1.pickle\", 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "source": [
    "## 비교할 이투데이 뉴스 idf 만들기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"idf_dit_all.pickle\", 'rb') as handle:\n",
    "    dit_1 = pickle.load(handle)  \n",
    "\n",
    "token_list_all = list(tfidf.vocabulary_.keys())\n",
    "\n",
    "cut_list = et_df['Contents'][100:150]\n",
    "vs_list = []\n",
    "for doc in cut_list:\n",
    "    vs_list.append([])\n",
    "    for token in token_list_all:\n",
    "        vs_list[-1].append(tf(token, doc)*dit_1[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "type(cut_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50, 65825)\n"
     ]
    }
   ],
   "source": [
    "dd = np.array(vs_list)\n",
    "print(dd.shape)"
   ]
  },
  {
   "source": [
    "## 코사인 유사도 계산\n",
    "\n",
    "=======>>> 모듈로 대체!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'Subject'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11876/820206706.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcosine_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvs_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcut_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Subject'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m66\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\news\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\news\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\news\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    386\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Subject'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_matrix = cosine_similarity([vs_list[30][:]], tfidf_matrix)\n",
    "print(cut_list['Subject'][66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 64122)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "cosine_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.09454831194582672\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in cosine_matrix[0]:\n",
    "    if a < i:\n",
    "        a = i\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news title과 id를 맵핑할 dictionary를 생성\n",
    "news2id = {}\n",
    "for i, c in enumerate(df['Subject']):\n",
    "    news2id[i] = c\n",
    "\n",
    "# id와 news title를 매핑할 dictionary를 생성\n",
    "id2news = {}\n",
    "for i, c in news2id.items():\n",
    "    id2news[c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 1105 is out of bounds for axis 0 with size 1",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11876/2923194378.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid2news\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'첫 정규앨범 블랙핑크 \"할 수 있는 음악 다 보여주고 싶었죠\"'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msim_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msim_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1105 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "idx = id2news['첫 정규앨범 블랙핑크 \"할 수 있는 음악 다 보여주고 싶었죠\"']\n",
    "sim_scores = [(i, c) for i, c in enumerate(cosine_matrix[idx])]\n",
    "sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scores[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scores = [(news2id[i], score) for i, score in sim_scores[0:5]]\n",
    "sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "sim = 0\n",
    "cnt = 0\n",
    "for i, tf_idf in enumerate(test_1000):\n",
    "    sim = cos_sim(vs_list[9], tf_idf)\n",
    "    if sim >= 0.6:\n",
    "        print(df['Subject'][i], sim)\n",
    "        print(et_df['Subject'][i])\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} 완료\".format(cnt))\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cos_sim(vs_list[5], tf_idf)"
   ]
  }
 ]
}